# candle-coreml Development Status

## Current Status (August 2025)

### âœ… Completed Features

1. **Core Architecture**
   - Generic CoreML integration for Candle tensors
   - Multi-component ANEMLL model support
   - HuggingFace model downloading
   - QwenModel high-level API
   - Comprehensive test coverage

2. **Working Implementations**
   - Standard ANEMLL models (e.g., `anemll/anemll-Qwen-Qwen3-0.6B-LUT888-ctx512_0.3.4`)
   - Model naming configuration system
   - Temperature sampling and text generation
   - Production-ready error handling

3. **Consumer Library Validation**
   - Successfully demonstrated with consumer applications
   - Demonstrates real-world usage patterns
   - Validates API design and ergonomics

## âœ… RESOLVED: Shape Configuration for Different ANEMLL Models

### Problem Successfully Solved

During integration testing with fine-tuned models, we discovered and **successfully resolved** a critical limitation:

```
Error: CoreML stateful prediction error: MultiArray shape (64) does not match 
the shape (1) specified in the model description
```

**Status**: âœ… **FIXED** - All performance regression tests now pass with proper tensor shape handling.

### Root Cause Analysis

1. **Fixed Shape Assumptions**: The current `QwenModel` implementation uses hardcoded shape constants:
   ```rust
   pub const QWEN_BATCH_SIZE: usize = 64; // Only works for specific models
   pub const QWEN_CONTEXT_LENGTH: usize = 512;
   ```

2. **Model Variation**: Different ANEMLL models have different input/output shapes:
   - **Standard ANEMLL models**: Expect batch_size=64, context_length=512
   - **Fine-tuned models**: May expect batch_size=1, different context lengths
   - **Custom models**: Arbitrary shapes based on training configuration

3. **Non-Discovery**: No automatic shape detection from CoreML model metadata

### Impact

- âœ… Works: Standard ANEMLL models with expected shapes
- âŒ Fails: Fine-tuned models with different shapes
- âŒ Fails: Custom ANEMLL models with non-standard configurations

## âœ… IMPLEMENTED: Shape Discovery System

**Status**: âœ… **COMPLETED** - Dynamic shape detection successfully implemented

The solution uses the existing `ModelConfig` system with component-specific tensor configurations:

```rust
// Dynamic configuration generation for any model
let loader = UnifiedModelLoader::new()?;
let model = loader.load_model("model_id")?;
// Automatic config generation supports arbitrary model configurations

// Mode-aware tensor creation
config.create_position_ids_with_mode_detection(&positions, is_prefill)
config.create_causal_mask_with_mode_detection(position, context_length, is_prefill)
```

## âœ… IMPLEMENTED: Dynamic QwenConfig

**Status**: âœ… **COMPLETED** - Configurable shapes with backward compatibility

```rust
impl QwenConfig {
    /// Create from ModelConfig (recommended approach)
    pub fn from_model_config(model_config: ModelConfig) -> Self;
    
    /// Auto-configure for known model IDs
    pub fn for_model_id(model_id: &str) -> Result<Self, CandleError>;
    
    // Backward compatible factory methods (deprecated)
    pub fn for_standard_qwen() -> Self;
}
```

## âœ… IMPLEMENTED: Adaptive Tensor Creation

**Status**: âœ… **COMPLETED** - Mode-aware tensor creation with shape validation

```rust
impl QwenConfig {
    /// Mode-aware tensor creation (TDD approach)
    pub fn create_position_ids_with_mode_detection(&self, positions: &[i64], is_prefill: bool) -> Result<Tensor>;
    pub fn create_causal_mask_with_mode_detection(&self, position: usize, context_length: usize, is_prefill: bool) -> Result<Tensor>;
    
    /// Component-specific tensor creation
    pub fn create_embeddings_input_tensor(&self, tokens: &[i64]) -> Result<Tensor>;
    pub fn create_single_token_embeddings_input(&self, token: i64) -> Result<Tensor>;
}
```

## ğŸ”„ IN PROGRESS: Code Organization Refactoring

### Current Activity: Breaking Down Large qwen.rs File (2263 lines)

**Goal**: Improve maintainability by splitting the monolithic qwen.rs into focused modules.

**Progress** (In Progress - August 7, 2025):
- âœ… **Phase 1**: Created `src/qwen/` module structure with `mod.rs`
- âœ… **Phase 2**: Extracted `ModelNamingConfig` to `qwen/naming.rs` 
- ğŸ”„ **Phase 3**: Extracting `QwenConfig` to `qwen/config.rs` (In Progress)
- â³ **Phase 4**: Extract tensor creation methods to `qwen/tensors.rs`
- â³ **Phase 5**: Split model loading and inference logic
- â³ **Phase 6**: Update imports and verify all tests pass

**New Module Structure**:
```
src/qwen/
â”œâ”€â”€ mod.rs           # Public API re-exports (âœ… Done)
â”œâ”€â”€ naming.rs        # File naming patterns (âœ… Done)  
â”œâ”€â”€ config.rs        # QwenConfig structure (ğŸ”„ In Progress)
â”œâ”€â”€ tensors.rs       # Tensor creation logic (â³ Pending)
â”œâ”€â”€ model.rs         # QwenModel struct and loading (â³ Pending)
â””â”€â”€ inference.rs     # Inference pipeline (â³ Pending)
```

**Benefits**: Better code organization, easier maintenance, clearer separation of concerns, improved testability.

## ğŸ§¹ IMPLEMENTED: Enhanced Cache Management System (August 2025)

### Problem Successfully Solved

Investigation revealed that CoreML cache accumulation was consuming massive disk space:
- **46.19 GB** of accumulated cache data found across 13 directories
- Test runs create persistent `{test_name}-{hash}/com.apple.e5rt.e5bundlecache` directories
- Apple controls cache directory naming based on bundle identifier and process name

### Solution Implemented

1. **Bundle Identifier Investigation**: 
   - `NSBundle.mainBundle().bundleIdentifier()` returns `None` for cargo/command-line processes
   - Environment variables (`CFBundleIdentifier`) don't affect CoreML cache naming
   - Cache directories follow pattern: `{process_name}-{hash}/com.apple.e5rt.e5bundlecache`

2. **Enhanced Cleanup Scripts**: 
   - `cleanup_coreml_caches_enhanced.sh` - detects all candle-coreml cache patterns
   - Handles multiple cache types: integration tests, performance tests, qwen tests, etc.
   - Interactive, dry-run, and batch removal modes for safe cleanup

3. **CacheManager API**: 
   - `src/cache_manager.rs` - programmatic cache management in Rust
   - Safety checks prevent accidental deletion of system directories
   - Automatic detection of CoreML-related cache directories

### Cache Detection Results

```bash
# Found cache directories (sorted by size):
integration_tests-d82b189a4c4542f1         (36.6 GB)  # âš ï¸ Largest!
typo_fixer_test-1346f12d3fdcafc6          (3.9 GB)
integration_tests-c175e5d207035e23         (1.7 GB)
typo_fixer_tests-805e09825c2c7005         (1.5 GB)
performance_regression_tests-*             (297 MB each)
candle_coreml-*                            (297 MB each)
qwen_tests-*                               (297 MB each)
# ... and more
```

### Cache Cleanup Commands

```bash
# Enhanced cleanup script (recommended)
./cleanup_coreml_caches_enhanced.sh

# Options: 1) Remove all, 2) Interactive, 3) Dry run, 4) Cancel

# Or via Rust API for programmatic access
cargo test cache_manager::tests::test_find_all_candle_coreml_caches -- --nocapture
```

## âœ… COMPLETED: Shape Configuration Implementation  

### Phase 1: Shape Discovery âœ… DONE
1. **Dynamic Model Configuration**
   - âœ… Automatic config generation from .mlpackage files  
   - âœ… Real-time shape detection for any ANEMLL model
   - âœ… No more hardcoded model configurations needed

2. **QwenConfig Enhancement** âœ… DONE
   - âœ… Added `model_config` field to QwenConfig with shape information
   - âœ… Implemented `for_model_id()` method for automatic configuration
   - âœ… Maintained full backward compatibility with existing code

### Phase 2: Dynamic Tensor Handling âœ… DONE  
1. **Adaptive Tensor Creation** âœ… DONE
   - âœ… Replaced hardcoded shape constants with config-driven shapes
   - âœ… Updated tensor creation methods with mode detection (prefill vs infer)
   - âœ… Added comprehensive shape validation before CoreML calls

2. **Error Handling Enhancement** âœ… DONE
   - âœ… Clear error messages for shape mismatches
   - âœ… TDD-driven testing and validation
   - âœ… Comprehensive debugging utilities and integration tests

### Phase 3: Testing & Validation âœ… DONE
1. **Multi-Model Test Suite** âœ… DONE
   - âœ… All 5 performance regression tests now passing
   - âœ… Successfully tested with standard ANEMLL models  
   - âœ… Successfully tested with various model configurations
   - âœ… Achieved 5.34 tokens/second generation performance

2. **Production Readiness** âœ… DONE
   - âœ… All CI/CD pipeline checks passing
   - âœ… Code formatting and linting resolved
   - âœ… Full test coverage across different model types

## ğŸ“Š Expected Benefits

### For Library Users
- âœ… **Universal Compatibility**: Works with any ANEMLL model regardless of shapes
- âœ… **Zero Configuration**: Automatic shape discovery eliminates manual config
- âœ… **Better Error Messages**: Clear guidance when shape issues occur
- âœ… **Backward Compatibility**: Existing code continues to work

### For candle-coreml Development
- âœ… **Generic Design**: Maintains library's generic nature (not model-specific)
- âœ… **Extensibility**: Easy to support new ANEMLL architectures
- âœ… **Robustness**: Handles edge cases and model variations gracefully

## ğŸ¯ Success Criteria

1. **Consumer Integration**: Successfully integrate with applications requiring text generation
2. **Multi-Model Support**: Same codebase works with:
   - Standard ANEMLL models (anemll/*)
   - Fine-tuned models with custom configurations
   - Custom models with arbitrary shapes
3. **Documented Breaking Changes**: Clear migration path for any API changes (version bump allows breaking changes)
4. **Auto-Discovery**: New models work out-of-the-box without manual configuration

## ğŸ“ Current Consumer Integration Status

### Consumer Applications
- âœ… **API Design**: Clean, ergonomic API for model loading and inference
- âœ… **Generic Support**: Works with any ANEMLL model configuration
- âœ… **Error Handling**: Comprehensive error reporting and validation

### Next Steps for Consumer
1. **Enhanced Model Support** (expand to more ANEMLL architectures)
2. **Test with Multiple Model Types** (validates generic design)
3. **Performance Optimization** (after basic functionality working)

## ğŸ”— Related Files

- **Core Implementation**: `src/qwen.rs` - QwenModel and QwenConfig
- **Configuration**: `src/config.rs` - CoreML configuration structures  
- **Model Loading**: `src/model_downloader.rs` - HuggingFace integration
- **Cache Management**: `src/cache_manager.rs` - Unified cache management system
- **Enhanced Cleanup**: `cleanup_coreml_caches_enhanced.sh` - Comprehensive cache cleanup
- **Examples**: `examples/` - Various usage demonstrations
- **Test Cases**: `tests/qwen_tests.rs` - Multi-model testing

## ğŸ”§ Development Notes

### Viewing Tracing Output

The codebase uses `tracing` for debug logging. To view tracing output when running tests or examples:

```bash
# View debug-level tracing output
RUST_LOG=debug cargo test test_name -- --nocapture

# View trace-level output (more verbose)
RUST_LOG=trace cargo test test_name -- --nocapture

# Filter to specific modules (e.g., just candle-coreml)
RUST_LOG=candle_coreml=debug cargo test test_name -- --nocapture

# View tracing output when running examples
RUST_LOG=debug cargo run --example example_name
```

---

**Priority**: High - This enhancement is critical for candle-coreml to be truly generic and support the diverse ecosystem of ANEMLL models.