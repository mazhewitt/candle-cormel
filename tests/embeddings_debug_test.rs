//! Embeddings Debug Test - Capture QwenModel embeddings vs Python
//!
//! This test captures the exact embeddings generated by our QwenModel
//! and compares them with the Python reference to find the mismatch.

use anyhow::Result;
use candle_core::{Device, Tensor};
use candle_coreml::{ensure_model_downloaded, qwen::{QwenModel, QwenConfig}};
use std::fs::File;
use std::io::{Read, Write};

/// Load a numpy array from .npy file (handles float16 and float32)
fn load_numpy_tensor(path: &str, expected_shape: &[usize], device: &Device) -> Result<Tensor> {
    let mut file = File::open(path)?;
    let mut buffer = Vec::new();
    file.read_to_end(&mut buffer)?;
    
    let data_start = if buffer.starts_with(b"\x93NUMPY") {
        let header_len = u16::from_le_bytes([buffer[8], buffer[9]]) as usize;
        10 + header_len
    } else {
        return Err(anyhow::Error::msg("Invalid .npy file format"));
    };
    
    let float_data = &buffer[data_start..];
    let expected_elements: usize = expected_shape.iter().product();
    
    let values = if float_data.len() == expected_elements * 2 {
        // float16 data
        let mut values = Vec::with_capacity(expected_elements);
        for chunk in float_data.chunks_exact(2) {
            let half_bits = u16::from_le_bytes([chunk[0], chunk[1]]);
            values.push(half_to_f32(half_bits));
        }
        values
    } else if float_data.len() == expected_elements * 4 {
        // float32 data
        let mut values = Vec::with_capacity(expected_elements);
        for chunk in float_data.chunks_exact(4) {
            let bytes = [chunk[0], chunk[1], chunk[2], chunk[3]];
            values.push(f32::from_le_bytes(bytes));
        }
        values
    } else {
        return Err(anyhow::Error::msg(format!(
            "Data size mismatch: expected {} elements, got {} bytes",
            expected_elements, float_data.len()
        )));
    };
    
    Tensor::from_vec(values, expected_shape, device).map_err(Into::into)
}

/// Convert float16 to float32
fn half_to_f32(half: u16) -> f32 {
    let sign = (half >> 15) & 0x1;
    let exp = (half >> 10) & 0x1f;
    let frac = half & 0x3ff;
    
    if exp == 0 {
        if frac == 0 {
            return if sign == 0 { 0.0 } else { -0.0 };
        } else {
            let f32_exp = -14i32;
            let f32_frac = (frac as f32) / 1024.0;
            let value = f32_frac * 2.0f32.powi(f32_exp);
            return if sign == 0 { value } else { -value };
        }
    } else if exp == 31 {
        if frac == 0 {
            return if sign == 0 { f32::INFINITY } else { f32::NEG_INFINITY };
        } else {
            return f32::NAN;
        }
    } else {
        let f32_exp = (exp as i32) - 15 + 127;
        let f32_frac = ((frac as u32) | 0x400) << 13;
        let f32_bits = ((sign as u32) << 31) | ((f32_exp as u32) << 23) | (f32_frac & 0x7fffff);
        return f32::from_bits(f32_bits);
    }
}

/// Save tensor as numpy array for comparison
fn save_tensor_as_numpy(tensor: &Tensor, path: &str) -> Result<()> {
    let vec_data = tensor.to_vec3::<f32>()?;
    let shape = tensor.shape();
    
    // Create simple numpy header
    let header = format!("{{'descr': '<f4', 'fortran_order': False, 'shape': {:?}, }}", 
                        (shape.dims()[0], shape.dims()[1], shape.dims()[2]));
    let header_len = header.len();
    let padding = (16 - ((10 + header_len) % 16)) % 16;
    let padded_header = format!("{}{}\\n", header, " ".repeat(padding));
    
    let mut file = File::create(path)?;
    file.write_all(b"\\x93NUMPY")?;
    file.write_all(&[0x01, 0x00])?; // Version
    file.write_all(&(padded_header.len() as u16).to_le_bytes())?;
    file.write_all(padded_header.as_bytes())?;
    
    // Write float32 data
    for batch in &vec_data {
        for seq in batch {
            for &val in seq {
                file.write_all(&val.to_le_bytes())?;
            }
        }
    }
    
    Ok(())
}

#[cfg(target_os = "macos")]
#[tokio::test]
#[ignore = "manual debug test"]
async fn test_embeddings_comparison() -> Result<()> {
    println!("üîç EMBEDDINGS DEBUG: Comparing QwenModel vs Python embeddings");
    
    let model_id = "anemll/anemll-Qwen-Qwen3-0.6B-LUT888-ctx512_0.3.4";
    let model_dir = ensure_model_downloaded(model_id, true)?;
    
    let config = QwenConfig::default();
    let mut qwen_model = QwenModel::load_from_directory(&model_dir, Some(config))?;
    
    println!("‚úÖ QwenModel loaded");
    
    let prompt = "The quick brown fox jumps over the lazy";
    println!("üìù Testing prompt: '{}'", prompt);
    
    // STEP 1: Get QwenModel embeddings
    println!("üîÑ Getting QwenModel embeddings...");
    let tokens = qwen_model.tokenize(prompt)?;
    println!("  Tokens: {:?} (length: {})", tokens, tokens.len());
    
    let rust_embeddings = qwen_model.compute_embeddings(&tokens)?;
    println!("  Rust embeddings shape: {:?}", rust_embeddings.shape());
    
    // Save Rust embeddings for comparison
    save_tensor_as_numpy(&rust_embeddings, "rust_embeddings.npy")?;
    println!("üíæ Saved Rust embeddings to rust_embeddings.npy");
    
    // STEP 2: Load Python reference embeddings
    let test_tensors_dir = "test_tensors";
    let device = Device::Cpu;
    
    if std::path::Path::new(&format!("{}/03_ffn_prefill_hidden_input.npy", test_tensors_dir)).exists() {
        println!("üì• Loading Python reference embeddings...");
        let python_embeddings = load_numpy_tensor(
            &format!("{}/03_ffn_prefill_hidden_input.npy", test_tensors_dir),
            &[1, 64, 1024],
            &device
        )?;
        println!("  Python embeddings shape: {:?}", python_embeddings.shape());
        
        // STEP 3: Compare embeddings
        println!("üìä CRITICAL COMPARISON: Rust vs Python embeddings");
        
        let rust_vec = rust_embeddings.to_vec3::<f32>()?;
        let python_vec = python_embeddings.to_vec3::<f32>()?;
        
        let mut max_diff = 0.0f32;
        let mut total_diff = 0.0f32;
        let mut num_elements = 0;
        let tokens_len = tokens.len();
        
        // Compare only the actual token embeddings (not padding)
        for i in 0..tokens_len.min(64) {
            for j in 0..1024 {
                let rust_val = rust_vec[0][i][j];
                let python_val = python_vec[0][i][j];
                let diff = (rust_val - python_val).abs();
                max_diff = max_diff.max(diff);
                total_diff += diff;
                num_elements += 1;
            }
        }
        
        let mean_diff = total_diff / num_elements as f32;
        
        println!("üìà EMBEDDINGS COMPARISON RESULTS:");
        println!("  Max difference: {:.8}", max_diff);
        println!("  Mean difference: {:.8}", mean_diff);
        println!("  Compared elements: {} (first {} tokens)", num_elements, tokens_len);
        
        // Show first few values for verification
        println!("\\nüîç First 10 values comparison:");
        println!("  Rust[0][0][0:10]: {:?}", &rust_vec[0][0][0..10.min(rust_vec[0][0].len())]);
        println!("  Python[0][0][0:10]: {:?}", &python_vec[0][0][0..10.min(python_vec[0][0].len())]);
        
        if max_diff < 1e-6 {
            println!("‚úÖ EMBEDDINGS MATCH! Difference is negligible");
        } else if max_diff < 0.01 {
            println!("‚ö†Ô∏è EMBEDDINGS CLOSE but not identical (max diff: {:.8})", max_diff);
        } else {
            println!("‚ùå EMBEDDINGS SIGNIFICANTLY DIFFERENT (max diff: {:.8})", max_diff);
            println!("   This explains why we don't get 'dog' prediction!");
        }
        
    } else {
        println!("‚ö†Ô∏è Python reference embeddings not found - run qwen-chat-test.py first");
    }
    
    // STEP 4: Test with the exact Python embeddings (if available)
    if std::path::Path::new(&format!("{}/03_ffn_prefill_hidden_input.npy", test_tensors_dir)).exists() {
        println!("\\nüß™ Testing QwenModel with Python embeddings...");
        
        let python_embeddings = load_numpy_tensor(
            &format!("{}/03_ffn_prefill_hidden_input.npy", test_tensors_dir),
            &[1, 64, 1024],
            &device
        )?;
        
        // Run the rest of the pipeline with Python embeddings
        qwen_model.reset_states()?;
        qwen_model.run_prefill_phase(&python_embeddings, tokens.len())?;
        
        // Get last token embedding from Python data  
        let last_token_tensor = Tensor::from_vec(vec![tokens[tokens.len() - 1]], (1, 1), &Device::Cpu)?;
        let last_token_embedding = qwen_model.embeddings.forward(&[&last_token_tensor])?;
        
        let logits = qwen_model.generate_next_token_with_infer(&last_token_embedding, tokens.len())?;
        
        // Extract token
        let flat_logits = logits.squeeze(0)?.squeeze(0)?;
        let logits_vec = flat_logits.to_vec1::<f32>()?;
        let next_token = logits_vec
            .iter()
            .enumerate()
            .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
            .map(|(i, _)| i as i64)
            .unwrap();
        
        println!("üéØ With Python embeddings: Generated token {} (should be 5562 for 'dog')", next_token);
        
        if next_token == 5562 {
            println!("‚úÖ SUCCESS! Using Python embeddings produces 'dog'!");
            println!("   The issue is in embedding generation, not architecture!");
        } else {
            println!("‚ùå Still wrong token - issue might be deeper");
        }
    }
    
    Ok(())
}

#[cfg(not(target_os = "macos"))]
#[tokio::test]
async fn test_embeddings_comparison_macos_requirement() {
    println!("‚ùå Embeddings comparison test requires macOS");
}