//! QwenModel Input Comparison - Compare generated inputs vs Python inputs
//!
//! This test compares the infer inputs generated by QwenModel helper methods
//! vs the exact inputs used by Python, to see if there are subtle differences.

use anyhow::Result;
use candle_core::{Device, Tensor};
use candle_coreml::{ensure_model_downloaded, qwen::{QwenModel, QwenConfig}};
use std::fs::File;
use std::io::Read;

/// Load numpy tensor (handles int32 and float data)
fn load_numpy_tensor(path: &str, expected_shape: &[usize], device: &Device) -> Result<Tensor> {
    let mut file = File::open(path)?;
    let mut buffer = Vec::new();
    file.read_to_end(&mut buffer)?;
    
    let data_start = if buffer.starts_with(b"\x93NUMPY") {
        let header_len = u16::from_le_bytes([buffer[8], buffer[9]]) as usize;
        10 + header_len
    } else {
        return Err(anyhow::Error::msg("Invalid .npy file format"));
    };
    
    let data = &buffer[data_start..];
    let expected_elements: usize = expected_shape.iter().product();
    
    if data.len() == expected_elements * 4 {
        if path.contains("input") || path.contains("token") || path.contains("position") {
            // int32 data
            let mut values = Vec::with_capacity(expected_elements);
            for chunk in data.chunks_exact(4) {
                let bytes = [chunk[0], chunk[1], chunk[2], chunk[3]];
                let int_val = i32::from_le_bytes(bytes);
                values.push(int_val as f32);
            }
            return Tensor::from_vec(values, expected_shape, device).map_err(Into::into);
        } else {
            // float32 data  
            let mut values = Vec::with_capacity(expected_elements);
            for chunk in data.chunks_exact(4) {
                let bytes = [chunk[0], chunk[1], chunk[2], chunk[3]];
                values.push(f32::from_le_bytes(bytes));
            }
            return Tensor::from_vec(values, expected_shape, device).map_err(Into::into);
        }
    } else if data.len() == expected_elements * 2 {
        // float16 data
        let mut values = Vec::with_capacity(expected_elements);
        for chunk in data.chunks_exact(2) {
            let half_bits = u16::from_le_bytes([chunk[0], chunk[1]]);
            values.push(half_to_f32(half_bits));
        }
        return Tensor::from_vec(values, expected_shape, device).map_err(Into::into);
    } else {
        return Err(anyhow::Error::msg(format!(
            "Data size mismatch: expected {} elements, got {} bytes",
            expected_elements, data.len()
        )));
    };
}

/// Convert float16 to float32
fn half_to_f32(half: u16) -> f32 {
    let sign = (half >> 15) & 0x1;
    let exp = (half >> 10) & 0x1f;
    let frac = half & 0x3ff;
    
    if exp == 0 {
        if frac == 0 {
            return if sign == 0 { 0.0 } else { -0.0 };
        } else {
            let f32_exp = -14i32;
            let f32_frac = (frac as f32) / 1024.0;
            let value = f32_frac * 2.0f32.powi(f32_exp);
            return if sign == 0 { value } else { -value };
        }
    } else if exp == 31 {
        if frac == 0 {
            return if sign == 0 { f32::INFINITY } else { f32::NEG_INFINITY };
        } else {
            return f32::NAN;
        }
    } else {
        let f32_exp = (exp as i32) - 15 + 127;
        let f32_frac = ((frac as u32) | 0x400) << 13;
        let f32_bits = ((sign as u32) << 31) | ((f32_exp as u32) << 23) | (f32_frac & 0x7fffff);
        return f32::from_bits(f32_bits);
    }
}

/// Compare tensors with detailed analysis
fn compare_tensors(name: &str, rust_tensor: &Tensor, python_tensor: &Tensor) -> Result<bool> {
    println!("üîç Comparing {}", name);
    println!("  Rust shape: {:?}, Python shape: {:?}", rust_tensor.shape(), python_tensor.shape());
    
    // Handle different ranks
    if rust_tensor.rank() != python_tensor.rank() {
        println!("  ‚ùå RANK MISMATCH: Rust rank {} vs Python rank {}", rust_tensor.rank(), python_tensor.rank());
        return Ok(false);
    }
    
    match rust_tensor.rank() {
        1 => {
            let rust_vec = rust_tensor.to_vec1::<f32>()?;
            let python_vec = python_tensor.to_vec1::<f32>()?;
            
            let mut max_diff = 0.0f32;
            for (r, p) in rust_vec.iter().zip(python_vec.iter()) {
                max_diff = max_diff.max((r - p).abs());
            }
            
            println!("  Max difference: {:.8}", max_diff);
            println!("  First 8 - Rust: {:?}", &rust_vec[0..8.min(rust_vec.len())]);
            println!("  First 8 - Python: {:?}", &python_vec[0..8.min(python_vec.len())]);
            
            if max_diff < 1e-6 {
                println!("  ‚úÖ MATCH!");
                return Ok(true);
            } else {
                println!("  ‚ùå DIFFER!");
                return Ok(false);
            }
        }
        4 => {
            // Flatten 4D tensors for comparison
            let rust_flat = rust_tensor.flatten_all()?.to_vec1::<f32>()?;
            let python_flat = python_tensor.flatten_all()?.to_vec1::<f32>()?;
            
            let mut max_diff = 0.0f32;
            let total_elements = rust_flat.len();
            
            for (r_val, p_val) in rust_flat.iter().zip(python_flat.iter()) {
                max_diff = max_diff.max((r_val - p_val).abs());
            }
            
            println!("  Max difference: {:.8} over {} elements", max_diff, total_elements);
            
            if max_diff < 1e-6 {
                println!("  ‚úÖ MATCH!");
                return Ok(true);
            } else {
                println!("  ‚ùå DIFFER!");
                // Show some example values
                println!("  Sample Rust[0:8]: {:?}", &rust_flat[0..8.min(rust_flat.len())]);
                println!("  Sample Python[0:8]: {:?}", &python_flat[0..8.min(python_flat.len())]);
                return Ok(false);
            }
        }
        _ => {
            return Err(anyhow::Error::msg(format!("Unsupported tensor rank: {}", rust_tensor.rank())));
        }
    }
}

#[cfg(target_os = "macos")]
#[tokio::test]
#[ignore = "requires test tensors"]
async fn test_qwen_input_comparison() -> Result<()> {
    println!("üî¨ QWEN INPUT COMPARISON - QwenModel generated vs Python inputs");
    
    let model_id = "anemll/anemll-Qwen-Qwen3-0.6B-LUT888-ctx512_0.3.4";
    let model_dir = ensure_model_downloaded(model_id, true)?;
    
    let config = QwenConfig::default();
    let qwen_model = QwenModel::load_from_directory(&model_dir, Some(config))?;
    
    let device = Device::Cpu;
    let current_position = 7; // Position of last token "lazy" (FIXED: was 8, should be 7 like Python)
    let context_length = qwen_model.config().context_length;
    
    println!("‚úÖ QwenModel loaded");
    
    // STEP 1: Generate QwenModel infer inputs
    println!("\\nü¶Ä STEP 1: Generating QwenModel infer inputs...");
    
    let rust_update_mask = qwen_model.create_update_mask(current_position, context_length)?;
    let rust_position_ids = Tensor::from_vec(vec![current_position as i64], (1,), &device)?;
    let rust_causal_mask = qwen_model.create_position_causal_mask(current_position, context_length)?;
    let rust_current_pos = rust_position_ids.clone();
    
    println!("  Generated update mask: {:?}", rust_update_mask.shape());
    println!("  Generated position IDs: {:?}", rust_position_ids.shape());
    println!("  Generated causal mask: {:?}", rust_causal_mask.shape());
    println!("  Generated current pos: {:?}", rust_current_pos.shape());
    
    // STEP 2: Load Python infer inputs
    println!("\\nüêç STEP 2: Loading Python infer inputs...");
    
    let python_update_mask = load_numpy_tensor("test_tensors/04_infer_update_mask.npy", &[1, 1, 512, 1], &device)?;
    let python_position_ids = load_numpy_tensor("test_tensors/04_infer_position_ids.npy", &[1], &device)?;
    let python_causal_mask = load_numpy_tensor("test_tensors/04_infer_causal_mask.npy", &[1, 1, 1, 512], &device)?;
    // Python current_pos is same as position_ids
    
    println!("  Python update mask: {:?}", python_update_mask.shape());
    println!("  Python position IDs: {:?}", python_position_ids.shape());
    println!("  Python causal mask: {:?}", python_causal_mask.shape());
    
    // STEP 3: Compare each input
    println!("\\nüìä STEP 3: DETAILED INPUT COMPARISON");
    
    let update_mask_match = compare_tensors("UPDATE MASK", &rust_update_mask, &python_update_mask)?;
    let position_ids_match = compare_tensors("POSITION IDS", &rust_position_ids, &python_position_ids)?;
    let causal_mask_match = compare_tensors("CAUSAL MASK", &rust_causal_mask, &python_causal_mask)?;
    
    // STEP 4: Summary
    println!("\\nüìã INPUT COMPARISON SUMMARY:");
    println!("  Update mask match: {}", if update_mask_match { "‚úÖ" } else { "‚ùå" });
    println!("  Position IDs match: {}", if position_ids_match { "‚úÖ" } else { "‚ùå" });
    println!("  Causal mask match: {}", if causal_mask_match { "‚úÖ" } else { "‚ùå" });
    
    if update_mask_match && position_ids_match && causal_mask_match {
        println!("\\nüéâ ALL INPUTS MATCH! The issue is not in input generation.");
        println!("   The problem must be elsewhere in the pipeline.");
    } else {
        println!("\\nüö® INPUT MISMATCH FOUND! This explains the wrong FFN output.");
        
        if !update_mask_match {
            println!("   ‚ùå Update mask generation is wrong");
        }
        if !position_ids_match {
            println!("   ‚ùå Position IDs generation is wrong");
        }
        if !causal_mask_match {
            println!("   ‚ùå Causal mask generation is wrong");
        }
        
        println!("\\nüîß Fix these input generation methods to match Python exactly!");
    }
    
    Ok(())
}

#[cfg(not(target_os = "macos"))]
#[tokio::test]
async fn test_qwen_input_comparison_macos_requirement() {
    println!("‚ùå QwenModel input comparison test requires macOS");
}