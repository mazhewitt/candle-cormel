//! Comprehensive Qwen Model Tests
//!
//! This file consolidates all Qwen-specific tests including:
//! - Architecture validation and pipeline testing
//! - Position fix validation and boundary conditions
//! - Specific prediction testing (dog completion)  
//! - End-to-end integration testing
//! - Extended coverage for edge cases
//!
//! Consolidates content from:
//! - qwen_architecture_success_test.rs
//! - qwen_position_fix_test.rs  
//! - rust_dog_prediction_test.rs
//! - qwen_integration_tests.rs
//! - comprehensive_qwen_tests.rs

use anyhow::Result;
use candle_coreml::{
    qwen::{QwenConfig, QwenModel},
    UnifiedModelLoader,
};

// Test constants - canonical test prompts and expected results
const QUICK_BROWN_FOX_PROMPT: &str = "The quick brown fox jumps over the lazy";
const EXPECTED_DOG_TOKEN: i64 = 5562; // The canonical "dog" token
const MODEL_ID: &str = "anemll/anemll-Qwen-Qwen3-0.6B-LUT888-ctx512_0.3.4";

// Helper function to check CoreML compatibility
fn check_coreml_compatibility() -> bool {
    use std::process::Command;

    // Check if we're in CI environment and skip CoreML version checks
    if std::env::var("CI").is_ok() || std::env::var("GITHUB_ACTIONS").is_ok() {
        println!("‚ö†Ô∏è  Running in CI environment - CoreML compatibility issues may occur");
        return false; // Skip CoreML tests in CI for now due to version issues
    }

    // Try to detect macOS version (basic check)
    if let Ok(output) = Command::new("sw_vers").arg("-productVersion").output() {
        if let Ok(version) = String::from_utf8(output.stdout) {
            println!("macOS version: {}", version.trim());
            // macOS 15+ has CoreML 9.0+, macOS 14 and below have older versions
            return version.starts_with("15.") || version.starts_with("16.");
        }
    }

    true // Default to true for local development
}

// Helper function: ensure the model is downloaded and return a loaded QwenModel
async fn create_test_model() -> anyhow::Result<QwenModel> {
    // Use UnifiedModelLoader which can also download the model if missing
    let loader = UnifiedModelLoader::new()?;
    // Proactively ensure the model is available (downloads if missing)
    let _ = loader.ensure_model_available(MODEL_ID)?;
    // Load the model using the generated or cached config
    let model = loader.load_model(MODEL_ID)?;
    Ok(model)
}

#[cfg(target_os = "macos")]
mod architecture_tests {
    use super::*;

    #[tokio::test]
    async fn test_qwen_architecture_success() -> Result<()> {
        if !check_coreml_compatibility() {
            println!("‚ö†Ô∏è Skipping CoreML test due to version compatibility issues");
            return Ok(());
        }

        println!("üéâ Testing fixed QwenModel architecture");

        // Use UnifiedModelLoader for automatic configuration
        let loader = UnifiedModelLoader::new()?;
        let mut qwen_model = loader.load_model(MODEL_ID)?;

        println!("‚úÖ QwenModel loaded successfully");

        // Test that the model can complete text without errors
        let test_prompts = ["Hello world", "The quick brown fox", "In the beginning"];

        for prompt in &test_prompts {
            println!("\nüìù Testing prompt: '{prompt}'");

            // This should work without panicking using our fixed architecture
            let result = qwen_model.forward_text(prompt);
            match result {
                Ok(token) => {
                    println!("üéØ Generated token: {token}");

                    // Try to decode the token
                    if let Ok(decoded) = qwen_model.tokenizer().decode(&[token as u32], false) {
                        println!("üìñ Decoded: '{decoded}'");
                    } else {
                        println!("‚ö†Ô∏è Token {token} exists but couldn't decode");
                    }

                    // Basic sanity check - token should be in valid range
                    assert!(
                        (0..200000).contains(&token),
                        "Token {token} should be in reasonable range"
                    );
                }
                Err(e) => {
                    panic!("‚ùå QwenModel failed: {e}");
                }
            }
        }

        println!("\nüéâ SUCCESS! QwenModel architecture is working correctly!");
        println!("   ‚úÖ Uses proper prefill‚Üíinfer pipeline");
        println!("   ‚úÖ Shares state between prefill and infer phases");
        println!("   ‚úÖ Generates tokens without errors");
        println!("   ‚úÖ Can handle multiple different prompts");

        Ok(())
    }
}

#[cfg(target_os = "macos")]
mod position_fix_tests {
    use super::*;

    #[tokio::test]
    async fn test_qwen_position_fix() -> Result<()> {
        if !check_coreml_compatibility() {
            println!("‚ö†Ô∏è Skipping CoreML test due to version compatibility issues");
            return Ok(());
        }

        println!("üîß Testing QwenModel with position fix");

        // Use UnifiedModelLoader for automatic configuration
        let loader = UnifiedModelLoader::new()?;
        let mut qwen_model = loader.load_model(MODEL_ID)?;

        println!("‚úÖ QwenModel loaded");

        // Test the exact prompt that should predict "dog"
        let prompt = QUICK_BROWN_FOX_PROMPT;
        println!("üìù Testing prompt: '{prompt}'");

        // Also test a longer prompt that would trigger the /no_think issue
        let longer_prompt = "Tell me about Greece. I want to know about its history, culture, and geography. What makes it special?";
        println!("üìù Also testing longer prompt: '{longer_prompt}'");

        let tokens = qwen_model
            .tokenizer()
            .encode(longer_prompt, true)
            .map_err(|e| anyhow::Error::msg(format!("Tokenization failed: {e}")))?;
        println!(
            "üî¢ Longer prompt tokenized to {} tokens",
            tokens.get_ids().len()
        );

        let next_token = qwen_model.forward_text(prompt)?;

        println!("üéØ Prediction: token {next_token}");

        // Test with the longer prompt (this would previously cause tensor indexing error)
        if tokens.get_ids().len() <= 50 {
            // Only test if within reasonable limits
            println!("üß™ Testing longer prompt (this would previously fail)...");
            match qwen_model.forward_text(longer_prompt) {
                Ok(long_token) => {
                    println!("‚úÖ Longer prompt works! Predicted token: {long_token}");
                    if let Ok(decoded) = qwen_model.tokenizer().decode(&[long_token as u32], false)
                    {
                        println!("üìñ Decoded: '{decoded}'");
                    }
                }
                Err(e) => {
                    println!("‚ö†Ô∏è  Longer prompt failed (expected for very long inputs): {e}");
                }
            }
        } else {
            println!(
                "‚è≠Ô∏è  Skipping longer prompt test (too long: {} tokens)",
                tokens.get_ids().len()
            );
        }

        // Decode the token
        if let Ok(decoded) = qwen_model.tokenizer().decode(&[next_token as u32], false) {
            println!("üìñ Decoded: '{decoded}'");
        }

        if next_token == EXPECTED_DOG_TOKEN {
            println!("‚úÖ üéâ SUCCESS! QwenModel now predicts 'dog' correctly!");
            Ok(())
        } else {
            println!(
                "‚ùå Still predicting token {next_token} instead of {EXPECTED_DOG_TOKEN} ('dog')"
            );

            // Check if it's at least different from the previous wrong prediction
            if next_token == 15678 {
                println!("   Still getting 'lazy' - position fix didn't help");
            } else {
                println!("   Different prediction - position fix may have changed something");
            }

            panic!(
                "POSITION FIX TEST FAILED: Expected token {} ('dog'), got token {} ('{}')",
                EXPECTED_DOG_TOKEN,
                next_token,
                qwen_model
                    .tokenizer()
                    .decode(&[next_token as u32], false)
                    .unwrap_or("???".to_string())
            );
        }
    }
}

#[cfg(target_os = "macos")]
mod prediction_tests {
    use super::*;

    #[tokio::test]
    async fn test_rust_dog_prediction() -> Result<()> {
        if !check_coreml_compatibility() {
            println!("‚ö†Ô∏è Skipping CoreML test due to version compatibility issues");
            return Ok(());
        }

        println!("üéØ Testing Rust QwenModel prediction for 'dog' completion");

        // Use UnifiedModelLoader for automatic configuration
        let loader = UnifiedModelLoader::new()?;
        let mut qwen_model = loader.load_model(MODEL_ID)?;

        let prompt = QUICK_BROWN_FOX_PROMPT;
        println!("üìù Prompt: '{prompt}'");

        let predicted_token = qwen_model.forward_text(prompt)?;
        let decoded = qwen_model
            .tokenizer()
            .decode(&[predicted_token as u32], false)
            .map_err(|e| anyhow::Error::msg(format!("Decode error: {e}")))?;

        println!("üéØ Rust prediction: Token {predicted_token} = '{decoded}'");

        // Check if it's "dog" (token 5562)
        if predicted_token == EXPECTED_DOG_TOKEN {
            println!("üéâ SUCCESS! Rust correctly predicts 'dog'");
        } else {
            println!(
                "‚ùå Different prediction. Expected: {EXPECTED_DOG_TOKEN} ('dog'), Got: {predicted_token} ('{decoded}')"
            );

            // Show what the tokenizer thinks about "dog"
            if let Ok(dog_tokens) = qwen_model.tokenizer().encode(" dog", false) {
                let dog_token_ids: Vec<u32> = dog_tokens.get_ids().to_vec();
                println!("üîç ' dog' tokenizes to: {dog_token_ids:?}");
            }
            panic!("Rust prediction did not match expected 'dog' token");
        }

        Ok(())
    }
}

#[cfg(target_os = "macos")]
mod integration_tests {
    use super::*;

    #[tokio::test]
    // Model is cached after first download - safe for coverage analysis
    async fn test_qwen_complete_pipeline_fox_completion() -> Result<()> {
        if !check_coreml_compatibility() {
            println!("‚ö†Ô∏è Skipping CoreML test due to version compatibility issues");
            return Ok(());
        }

        // Use UnifiedModelLoader for automatic model loading and configuration
        let loader = UnifiedModelLoader::new()?;
        let mut qwen_model = loader.load_model(MODEL_ID)?;

        // Test the classic "The quick brown fox jumps over the lazy" completion
        let prompt = QUICK_BROWN_FOX_PROMPT;

        // Generate completion with low temperature for deterministic results
        let completion = qwen_model.generate_text(prompt, 2, 0.0)?;

        // Assert that the completion contains "dog" or "lazy" (both have tied logits in the model)
        assert!(
            completion.to_lowercase().contains("dog") || completion.to_lowercase().contains("lazy"),
            "Expected completion to contain 'dog' or 'lazy' (both have tied logit values), but got: '{completion}'"
        );

        println!("‚úÖ Qwen pipeline test passed!");
        println!("Prompt: {prompt}");
        println!("Completion: {completion}");

        Ok(())
    }
}

#[cfg(target_os = "macos")]
mod extended_coverage_tests {
    use super::*;

    #[tokio::test]
    #[ignore] // Run with: cargo test test_state_management_edge_cases -- --ignored
    async fn test_state_management_edge_cases() -> Result<()> {
    let mut model = create_test_model().await?;

        // Test multiple state initializations (should not cause conflicts)
        for i in 1..=3 {
            println!("State initialization attempt {i}");

            // This should work repeatedly without issues
            let result = model.initialize_states();
            assert!(
                result.is_ok(),
                "State initialization {i} failed: {result:?}"
            );
        }

        println!("‚úÖ Multiple state initializations handled correctly");
        Ok(())
    }

    #[tokio::test]
    #[ignore] // Run with: cargo test test_cache_optimization_paths -- --ignored
    async fn test_cache_optimization_paths() -> Result<()> {
    let mut model = create_test_model().await?;

        let base_prompt = "The quick brown fox";

        // First call - should populate cache
        let _token1 = model.forward_text(base_prompt)?;

        // Second call with same prompt - should hit cache optimization
        let _token2 = model.forward_text(base_prompt)?;

        // Third call with extended prompt - should partially reuse cache
        let extended_prompt = "The quick brown fox jumps";
        let _token3 = model.forward_text(extended_prompt)?;

        // Fourth call with completely different prompt - should miss cache
        let different_prompt = "Artificial intelligence is";
        let _token4 = model.forward_text(different_prompt)?;

        println!("‚úÖ Cache optimization paths tested successfully");
        Ok(())
    }

    #[tokio::test]
    #[ignore] // Run with: cargo test test_generation_with_extreme_parameters -- --ignored
    async fn test_generation_with_extreme_parameters() -> Result<()> {
    let mut model = create_test_model().await?;

        let prompt = "The weather today is";

        // Test with temperature 0.0 (deterministic)
        let tokens_temp_0 = model.generate_tokens(prompt, 5, 0.0, None)?;
        assert!(
            !tokens_temp_0.is_empty(),
            "No tokens generated with temp 0.0"
        );

        // Test with high temperature
        let tokens_temp_high = model.generate_tokens(prompt, 5, 2.0, None)?;
        assert!(
            !tokens_temp_high.is_empty(),
            "No tokens generated with high temp"
        );

        // Test with 1 token generation
        let single_token = model.generate_tokens(prompt, 1, 0.5, None)?;
        assert_eq!(
            single_token.len(),
            1,
            "Expected exactly 1 token, got {}",
            single_token.len()
        );

        // Test with many tokens (within reason)
        let many_tokens = model.generate_tokens(prompt, 30, 0.5, None)?;
        assert!(
            !many_tokens.is_empty(),
            "No tokens generated with many token request"
        );

        println!("‚úÖ Extreme parameter testing completed");
        Ok(())
    }

    #[tokio::test]
    #[ignore] // Run with: cargo test test_custom_configurations -- --ignored
    async fn test_custom_configurations() -> Result<()> {
        // Test with UnifiedModelLoader - automatically handles configuration
        let loader = UnifiedModelLoader::new()?;
    // Ensure the model is available first (downloads if missing)
    let _ = loader.ensure_model_available(MODEL_ID)?;
        let mut model = loader.load_model(MODEL_ID)?;

        // Test that custom config model works
        let result = model.forward_text("Configuration test");
        match result {
            Ok(token) => println!("‚úÖ Generated token: {}", token),
            Err(e) => panic!("Custom config model failed to generate: {}", e),
        }

        println!("‚úÖ Custom configuration tested successfully");
        Ok(())
    }

    #[tokio::test]
    #[ignore] // Run with: cargo test test_conversation_continuity -- --ignored
    async fn test_conversation_continuity() -> Result<()> {
    let mut model = create_test_model().await?;

        // Simulate a conversation flow
        let conversation = [
            "Hello, how are you?",
            "What is artificial intelligence?",
            "Can you explain machine learning?",
            "Thank you for the information.",
        ];

        let mut responses = Vec::new();

        for (i, prompt) in conversation.iter().enumerate() {
            println!("Conversation turn {}: {}", i + 1, prompt);

            let response = model.forward_text(prompt);
            assert!(response.is_ok(), "Conversation turn {} failed", i + 1);

            let token = response?;
            responses.push(token);
            println!("  Response token: {token}");
        }

        // Verify we got different tokens (conversation progressed)
        assert!(responses.len() == conversation.len(), "Missing responses");

        println!(
            "‚úÖ Conversation continuity maintained across {} turns",
            responses.len()
        );
        Ok(())
    }
}

// Tests that can run without models
#[tokio::test]
async fn test_qwen_config_defaults() {
    let config = QwenConfig::default();

    // Test using new accessor methods instead of deprecated fields
    assert_eq!(config.vocab_size(), 151936); // Default Qwen vocab size
    assert_eq!(config.hidden_size(), 1024); // Default Qwen hidden size
    assert_eq!(config.batch_size(), 1); // Default batch size
    assert_eq!(config.context_length(), 512); // Default context length

    println!("‚úÖ QwenConfig defaults validated using new accessor methods");
}

#[tokio::test]
async fn test_model_config_system() {
    use candle_coreml::{ModelConfig, QwenConfig};

    // Test that we can create default model configurations
    let default_config = ModelConfig::default_qwen();
    assert_eq!(default_config.shapes.batch_size, 1);
    assert_eq!(default_config.shapes.context_length, 512);
    assert_eq!(default_config.shapes.hidden_size, 1024);

    // Test QwenConfig creation from ModelConfig
    let qwen_config = QwenConfig::from_model_config(default_config);
    assert_eq!(qwen_config.model_config.shapes.batch_size, 1);
    
    // Note: for dynamic model loading, use UnifiedModelLoader instead of builtin configs

    println!("‚úÖ Model config system validated");
}

#[cfg(not(target_os = "macos"))]
mod non_macos_tests {
    use super::*;
    use std::path::PathBuf;

    #[tokio::test]
    async fn test_qwen_requires_macos() {
        // Test that QwenModel appropriately requires macOS
        let fake_path = PathBuf::from("nonexistent");
        let result = QwenModel::load_from_directory(&fake_path, None);

        assert!(result.is_err(), "QwenModel should require macOS");

        let error_msg = result.unwrap_err().to_string();
        // Should mention macOS or CoreML requirement
        assert!(
            error_msg.to_lowercase().contains("macos")
                || error_msg.to_lowercase().contains("coreml")
                || error_msg.to_lowercase().contains("not found"),
            "Error should mention platform requirement: {}",
            error_msg
        );

        println!("‚úÖ macOS requirement properly enforced");
    }

    #[tokio::test]
    async fn test_qwen_macos_requirement() {
        // On non-macOS platforms, verify that appropriate errors are returned
        let model_dir = PathBuf::from("nonexistent_dir");
        let result = QwenModel::load_from_directory(&model_dir, None);

        // Should fail on non-macOS platforms
        assert!(
            result.is_err(),
            "QwenModel should not be available on non-macOS platforms"
        );
    }
}
